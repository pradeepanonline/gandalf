
# ğŸ§™â€â™‚ï¸ Gandalf's Principles â€“ Local RAG App

This is a fully local, offline AI assistant built with:

- âœ… Retrieval-Augmented Generation (RAG)
- âœ… JSON-based domain knowledge (e.g., architecture, coding principles)
- âœ… Open-source LLM (e.g., `llama3.2:latest` via [Ollama](https://ollama.com))
- âœ… Interactive UI via Streamlit

Designed to answer freeform questions based on your personal or enterprise principles â€” with no data ever leaving your machine.

---

## ğŸš€ Features

- ğŸ”’ 100% Offline â€“ all model inference and search is local
- ğŸ§  JSON-based knowledge base (RAG-ready)
- ğŸ” Embedding-based similarity search (cosine)
- ğŸ–¥ï¸ Runs with Ollama + Streamlit
- ğŸ§™â€â™‚ï¸ Custom branding (Gandalf banner!)
- ğŸ›  Easily swappable LLM (Deepseek, LLaMA, Mistral, etc.)

---

## ğŸ“ Folder Structure

```
.
â”œâ”€â”€ rag_streamlit.py           # Streamlit front-end (uses local LLM)
â”œâ”€â”€ embedder.py                # Generates embeddings from principles
â”œâ”€â”€ retriever.py               # Finds top matches based on similarity
â”œâ”€â”€ load_principles.py         # Loads JSON principle files
â”œâ”€â”€ principles/
â”‚   â”œâ”€â”€ architecture_principles.json
â”‚   â””â”€â”€ coding.json
â”œâ”€â”€ principles_embeddings.json # Generated by embedder.py
â”œâ”€â”€ README.md                  # You're here
```

---

## ğŸ”§ Requirements

- Python 3.9+
- Ollama (for local LLM inference)
- Streamlit
- NumPy
- scikit-learn
- requests

### Install dependencies:

```bash
pip install -r requirements.txt
```

**`requirements.txt`:**
```text
streamlit
numpy
scikit-learn
requests
```

---

## ğŸ§  Usage

### 1. Start your local LLM (via Ollama)

```bash
ollama run llama3.2:latest
```

Or use another model like:

```bash
ollama run deepseek-r1:14b
```

### 2. Embed your principles

```bash
python embedder.py
```

> This reads from `/principles/*.json` and generates `principles_embeddings.json`

### 3. Launch the app

```bash
streamlit run rag_streamlit.py
```

Then visit: `http://localhost:8501`

---

## ğŸ›¡ï¸ Privacy & Security

This app runs **100% locally**. No API calls are made to any external services.

- âœ… No OpenAI or cloud LLMs
- âœ… No telemetry
- âœ… Safe for internal documents and enterprise use

---

## ğŸ› ï¸ Configuration Options

You can customize:
- Principle domains (`principles/`)
- Top-K matches (in `retriever.py`)
- Model name (`llama3.2:latest`)
- Output formatting and citations

---

## ğŸ’¡ Future Ideas

- [ ] Export answers as PDF/Markdown
- [ ] Multi-turn Q&A (chat history)
- [ ] Stream output in real-time
- [ ] User feedback rating system
- [ ] Authentication for team access
- [ ] Deploy on intranet or internal GCP

---

## ğŸ“¸ Screenshot

![Gandalf RAG Screenshot]([https://i.imgur.com/7mjc4fO.jpg](https://cdn.pixabay.com/photo/2023/08/11/05/44/ai-generated-8182842_1280.jpg))

---

ğŸ§™â€â™‚ï¸ *"You shall not pass... without proper architectural justification."*
