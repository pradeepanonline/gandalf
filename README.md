
# 🧙‍♂️ Gandalf's Principles – Local RAG App

This is a fully local, offline AI assistant built with:

- ✅ Retrieval-Augmented Generation (RAG)
- ✅ JSON-based domain knowledge (e.g., architecture, coding principles)
- ✅ Open-source LLM (e.g., `llama3.2:latest` via [Ollama](https://ollama.com))
- ✅ Interactive UI via Streamlit

Designed to answer freeform questions based on your personal or enterprise principles — with no data ever leaving your machine.

---

## 🚀 Features

- 🔒 100% Offline – all model inference and search is local
- 🧠 JSON-based knowledge base (RAG-ready)
- 🔍 Embedding-based similarity search (cosine)
- 🖥️ Runs with Ollama + Streamlit
- 🧙‍♂️ Custom branding (Gandalf banner!)
- 🛠 Easily swappable LLM (Deepseek, LLaMA, Mistral, etc.)

---

## 📁 Folder Structure

```
.
├── rag_streamlit.py           # Streamlit front-end (uses local LLM)
├── embedder.py                # Generates embeddings from principles
├── retriever.py               # Finds top matches based on similarity
├── load_principles.py         # Loads JSON principle files
├── principles/
│   ├── architecture_principles.json
│   └── coding.json
├── principles_embeddings.json # Generated by embedder.py
├── README.md                  # You're here
```

---

## 🔧 Requirements

- Python 3.9+
- Ollama (for local LLM inference)
- Streamlit
- NumPy
- scikit-learn
- requests

### Install dependencies:

```bash
pip install -r requirements.txt
```

**`requirements.txt`:**
```text
streamlit
numpy
scikit-learn
requests
```

---

## 🧠 Usage

### 1. Start your local LLM (via Ollama)

```bash
ollama run llama3.2:latest
```

Or use another model like:

```bash
ollama run deepseek-r1:14b
```

### 2. Embed your principles

```bash
python embedder.py
```

> This reads from `/principles/*.json` and generates `principles_embeddings.json`

### 3. Launch the app

```bash
streamlit run rag_streamlit.py
```

Then visit: `http://localhost:8501`

---

## 🛡️ Privacy & Security

This app runs **100% locally**. No API calls are made to any external services.

- ✅ No OpenAI or cloud LLMs
- ✅ No telemetry
- ✅ Safe for internal documents and enterprise use

---

## 🛠️ Configuration Options

You can customize:
- Principle domains (`principles/`)
- Top-K matches (in `retriever.py`)
- Model name (`llama3.2:latest`)
- Output formatting and citations

---

## 💡 Future Ideas

- [ ] Export answers as PDF/Markdown
- [ ] Multi-turn Q&A (chat history)
- [ ] Stream output in real-time
- [ ] User feedback rating system
- [ ] Authentication for team access
- [ ] Deploy on intranet or internal GCP

---

## 📸 Screenshot

![Gandalf RAG Screenshot]([https://i.imgur.com/7mjc4fO.jpg](https://cdn.pixabay.com/photo/2023/08/11/05/44/ai-generated-8182842_1280.jpg))

---

🧙‍♂️ *"You shall not pass... without proper architectural justification."*
